# Crash Test
Reposit√≥rio utilizado para os c√≥digos do trabalho semestral da disciplina ECM514 - Ci√™ncia de Dados. 

<!---
COMO CRIAR UM SUMARIO/TABLE OF CONTENTS
the Markdown All in One plugin (extension).

To install it, launch the VS Code Quick Open (Control/‚åò+P), paste the following command, and press enter.

ext install yzhang.markdown-all-in-one

To generate the TOC, open the command palette (Control/‚åò+Shift+P) and select the Markdown All in One: Create Table of Contents option.
-->
- [Crash Test](#crash-test)
  - [üßëüèª‚Äçüíª Integrantes](#-integrantes)
  - [üö© Sobre o Projeto](#-sobre-o-projeto)
- [üîßDocumenta√ß√£o do Projeto](#documenta√ß√£o-do-projeto)
  - [üìå scrappingNew.py](#-scrappingnewpy)
    - [Descri√ß√£o](#descri√ß√£o)
    - [Funcionalidades](#funcionalidades)
    - [Depend√™ncias](#depend√™ncias)
    - [Estrutura do Script](#estrutura-do-script)
      - [Fun√ß√£o `criar_df(url)`](#fun√ß√£o-criar_dfurl)
      - [Processamento Paralelo com `ThreadPoolExecutor`](#processamento-paralelo-com-threadpoolexecutor)
    - [Sa√≠da e Armazenamento dos Dados:](#sa√≠da-e-armazenamento-dos-dados)
- [üìä Conclus√µes](#-conclus√µes)
  - [üöÄ Funcionamento](#-funcionamento)
  - [üìΩÔ∏è Video](#Ô∏è-video)
  - [üìù Artigo](#-artigo)
  - [üõú Links do Colab](#-links-do-colab)


## üßëüèª‚Äçüíª Integrantes
Nome: Caio Rabinovich Panes Brunholi 		RA: 20.01285-3 

Nome: Felippe Onishi Yaegashi 				RA: 20.00255-6

Nome: Jonathan Martins Gomes  			    RA: 20.00862-7

Nome: Matheus Marins Bernardello 			RA: 20.00286-6

## üö© Sobre o Projeto 
O projeto escolhido √© do tipo 2, Tabular Data Classification
Ele ir√° focar no uso de dados obtidos a partir de web scraping e/ou uso de APIs, focando nos m√©todos tradicionais de Machine Learning para classifica√ß√£o de dados.

O projeto baseia-se na obten√ß√£o de dados de dados de colis√£o de ve√≠culos (crash test) utilizando t√©cnicas de web scraping e/ou uso de APIs dos √≥rg√£o classificadores, mais especiificamente o [EURONCAP](https://www.euroncap.com/en).

>O EURONCAP foi escolhido, pois ele tem publicado em seu website o teste de mais ve√≠culos que o LATINCAP, e possui uma formata√ß√£o mais padronizada e completa que os demais √≥rg√£os classificadores.

Esses √≥rg√£os realizam testes de colis√£o e publicam os resultados, assim como os dados t√©cnicos dos ve√≠culos. Ap√≥s os testes, o estado dos ve√≠culos e dos bonecos de teste de colis√£o (crash dummies), tamb√©m conhecidos como Dispositivos Antropom√≥rficos de Teste s√£o analisados. 

Ent√£o o ve√≠culo recebe uma **classifica√ß√£o geral de seguran√ßa categ√≥rica, as estrelas, que variam de 0 a 5, sendo 0 um ve√≠culo pouco seguro e 5 um altamente seguro**.
O objetivo do projeto √© analisar os dados hist√≥ricos dos testes, relacionando, por exemplo, a marca, tipo de ve√≠culo, especifica√ß√µes t√©cnicas, entre outros, e conseguir prever as classifica√ß√µes de novos modelos de carros antes da realiza√ß√£o dos ensaios destrutivos.

O desenvolvimento ser√° realizado utilizando o Google Colab e demais ferramentas auxiliares conforme a necessidade, fornecendo ao usu√°rio final uma interface em que ele poder√° inserir os dados do novo ve√≠culo e obter sua classifica√ß√£o estimada (quantas estrelas ele ter√°).

</br>

# üîßDocumenta√ß√£o do Projeto
O Projeto √© composto de 3 arquivos principais:
- scrappingNew.py
- CD.ipynb
- StreamlitCD.ipynb

Os arquivos `.ipynb` s√£o os notebooks que podem ser executados diretamente no Google Colab. J√° o arquivo `.py` √© um script que deve ser executado diretamente. Para isso √© necess√°rio instalar as bibliotecas:
```bash
pip install -r /path/to/requirements.txt
```  

## üìå scrappingNew.py
### Descri√ß√£o
Este script realiza web scraping no site do EURONCAP para coletar dados estruturados de elementos HTML espec√≠ficos. Ele usa a biblioteca `requests` para fazer requisi√ß√µes HTTP e o `BeautifulSoup` para processar o HTML e extrair informa√ß√µes relevantes. Os dados coletados s√£o organizados em um DataFrame do `pandas`, facilitando a an√°lise e exporta√ß√£o para outros formatos de dados. Al√©m disso, o script utiliza o `ThreadPoolExecutor` para realizar m√∫ltiplas requisi√ß√µes de forma paralela, melhorando a efici√™ncia e diminuindo o tempo de execu√ß√£o ao processar grandes conjuntos de URLs.

### Funcionalidades
- **Requisi√ß√£o de P√°ginas da Web:** Para cada URL fornecida, o script envia uma requisi√ß√£o HTTP, obtendo o conte√∫do da p√°gina HTML.
- **Parsing do HTML:** Com o `BeautifulSoup`, o script identifica elementos HTML de interesse (por exemplo, `div` com uma classe espec√≠fica) e extrai o conte√∫do desses elementos.
- **Armazenamento de Dados:** Os dados coletados s√£o organizados em um DataFrame do `pandas` para facilitar an√°lises e manipula√ß√µes adicionais.
- **Processamento Paralelo:** Atrav√©s do `ThreadPoolExecutor`, o script executa m√∫ltiplas requisi√ß√µes simultaneamente, o que √© particularmente √∫til quando se lida com um grande n√∫mero de URLs.

### Depend√™ncias
O script requer as seguintes bibliotecas Python:
- `requests` para realizar requisi√ß√µes HTTP.
- `beautifulsoup4` para parsing e extra√ß√£o de dados do HTML.
- `pandas` para organiza√ß√£o e manipula√ß√£o dos dados extra√≠dos.
- 
### Estrutura do Script

O script importa as bibliotecas necess√°rias e define uma fun√ß√£o chamada `criar_df`, que √© respons√°vel pela maior parte do processo de scraping e extra√ß√£o de dados. Al√©m disso, ele define o cabe√ßalho `User-Agent` para simular um navegador e minimizar bloqueios.

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
from concurrent.futures import ThreadPoolExecutor
```

#### Fun√ß√£o `criar_df(url)`

Esta fun√ß√£o √© o n√∫cleo do script. Ela executa os seguintes passos:

1. **Log de In√≠cio:** Exibe uma mensagem de log indicando o in√≠cio do processamento da URL, √∫til para monitoramento e debug.

2. **Configura√ß√£o do User-Agent:** Define um `User-Agent` para a requisi√ß√£o, simulando um navegador real e evitando bloqueios comuns durante o scraping.

    ```python
    headers = {'User-Agent': "Mozilla/5.0 ... Safari/537.36"}
    ```

3. **Requisi√ß√£o HTTP:** Envia uma requisi√ß√£o GET para a URL usando a biblioteca `requests`, passando o cabe√ßalho com o `User-Agent`.

    ```python
    request = requests.get(url, headers=headers)
    ```

4. **Parsing do HTML:** Ap√≥s receber a resposta, o conte√∫do HTML da p√°gina √© analisado pelo `BeautifulSoup`. A fun√ß√£o `soup.find_all` √© usada para localizar elementos espec√≠ficos de interesse (por exemplo, `div` com classes espec√≠ficas).

    ```python
    soup = BeautifulSoup(request.content, 'html.parser')
    atributos1 = soup.find_all('div', class_='...')
    ```

5. **Extra√ß√£o de Dados:** Utiliza seletores de HTML espec√≠ficos para coletar os dados desejados de cada elemento HTML identificado.

6. **Estrutura√ß√£o dos Dados:** Organiza os dados coletados em uma estrutura apropriada e os insere em um DataFrame do `pandas` para facilitar a manipula√ß√£o e an√°lise.

#### Processamento Paralelo com `ThreadPoolExecutor`

Para processar v√°rias URLs simultaneamente, o script usa `ThreadPoolExecutor`, que executa m√∫ltiplas inst√¢ncias da fun√ß√£o `criar_df` em paralelo. Isso permite que o script fa√ßa scraping em v√°rias p√°ginas da web ao mesmo tempo, economizando tempo em compara√ß√£o com uma execu√ß√£o sequencial.

```python
with ThreadPoolExecutor(max_workers=5) as executor:
    executor.map(criar_df, urls)
```

Neste exemplo, `max_workers=5` limita o n√∫mero de threads a 5, mas este valor pode ser ajustado com base na quantidade de URLs e na capacidade do sistema.


### Sa√≠da e Armazenamento dos Dados: 
O DataFrame resultante √© r exportado para um arquivo CSV, permitindo f√°cil integra√ß√£o com outras ferramentas e softwares de an√°lise de dados.

</br>

# üìä Conclus√µes
## üöÄ Funcionamento
![alt text](images/image.png)
![alt text](images/image-1.png)

## üìΩÔ∏è Video

## üìù Artigo

## üõú Links do Colab

[Colab](https://colab.research.google.com/drive/1TCuvs70iniyzbc2-su6D5rB1eK1FduuT#scrollTo=NXIfPvveenWZ)


[Colab + Streamlit](https://colab.research.google.com/drive/1uKFoF86mV_WHvfIHBgtEZboCqDZWK69-?usp=sharing#scrollTo=D5yN_vbT80-p)
