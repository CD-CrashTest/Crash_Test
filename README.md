# Crash Test
Reposit√≥rio utilizado para os c√≥digos do trabalho semestral da disciplina ECM514 - Ci√™ncia de Dados. 

<!---
COMO CRIAR UM SUMARIO/TABLE OF CONTENTS
the Markdown All in One plugin (extension).

To install it, launch the VS Code Quick Open (Control/‚åò+P), paste the following command, and press enter.

ext install yzhang.markdown-all-in-one

To generate the TOC, open the command palette (Control/‚åò+Shift+P) and select the Markdown All in One: Create Table of Contents option.
-->

- [Crash Test](#crash-test)
  - [üßëüèª‚Äçüíª Integrantes](#-integrantes)
  - [üö© Sobre o Projeto](#-sobre-o-projeto)
- [üîßDocumenta√ß√£o do Projeto](#documenta√ß√£o-do-projeto)
  - [üìå scrappingNew.py](#-scrappingnewpy)
    - [Descri√ß√£o](#descri√ß√£o)
    - [Funcionalidades](#funcionalidades)
    - [Depend√™ncias](#depend√™ncias)
    - [Estrutura do Script](#estrutura-do-script)
      - [Fun√ß√£o `criar_df(url)`](#fun√ß√£o-criar_dfurl)
      - [Processamento Paralelo com `ThreadPoolExecutor`](#processamento-paralelo-com-threadpoolexecutor)
    - [Sa√≠da e Armazenamento dos Dados:](#sa√≠da-e-armazenamento-dos-dados)
  - [üìå CD.ipynb](#-cdipynb)
    - [Descri√ß√£o](#descri√ß√£o-1)
      - [Funcionalidades](#funcionalidades-1)
    - [Estrutura do Notebook](#estrutura-do-notebook)
    - [Escolha do Modelo](#escolha-do-modelo)
    - [Fonte dos Dados](#fonte-dos-dados)
  - [üìå StreamlitCD_RF_Keras.ipynb](#-streamlitcd_rf_kerasipynb)
    - [Descri√ß√£o](#descri√ß√£o-2)
    - [Funcionalidades](#funcionalidades-2)
    - [Estrutura](#estrutura)
- [üìä Conclus√µes](#-conclus√µes)
  - [üöÄ Funcionamento e Como Utilizar](#-funcionamento-e-como-utilizar)
  - [üìΩÔ∏è Video](#Ô∏è-video)
  - [üìù Artigo](#-artigo)
  - [üõú Links do Colab](#-links-do-colab)


## üßëüèª‚Äçüíª Integrantes
Nome: Caio Rabinovich Panes Brunholi 		RA: 20.01285-3 

Nome: Felippe Onishi Yaegashi 				RA: 20.00255-6

Nome: Jonathan Martins Gomes  			    RA: 20.00862-7

Nome: Matheus Marins Bernardello 			RA: 20.00286-6

## üö© Sobre o Projeto 
O projeto escolhido √© do tipo 2, Tabular Data Classification
Ele ir√° focar no uso de dados obtidos a partir de web scraping e/ou uso de APIs, focando nos m√©todos tradicionais de Machine Learning para classifica√ß√£o de dados.

O projeto baseia-se na obten√ß√£o de dados de dados de colis√£o de ve√≠culos (crash test) utilizando t√©cnicas de web scraping e/ou uso de APIs dos √≥rg√£o classificadores, mais especificamente o [EURONCAP](https://www.euroncap.com/en).

>O EURONCAP foi escolhido, pois ele tem publicado em seu website o teste de mais ve√≠culos que o LATINCAP, e possui uma formata√ß√£o mais padronizada e completa que os demais √≥rg√£os classificadores.

Esses √≥rg√£os realizam testes de colis√£o e publicam os resultados, assim como os dados t√©cnicos dos ve√≠culos. Ap√≥s os testes, o estado dos ve√≠culos e dos bonecos de teste de colis√£o (crash dummies), tamb√©m conhecidos como Dispositivos Antropom√≥rficos de Teste s√£o analisados. 

Ent√£o o ve√≠culo recebe uma **classifica√ß√£o geral de seguran√ßa categ√≥rica, as estrelas, que variam de 0 a 5, sendo 0 um ve√≠culo pouco seguro e 5 um altamente seguro**.
O objetivo do projeto √© analisar os dados hist√≥ricos dos testes, relacionando, por exemplo, a marca, tipo de ve√≠culo, especifica√ß√µes t√©cnicas, entre outros, e conseguir prever as classifica√ß√µes de novos modelos de carros antes da realiza√ß√£o dos ensaios destrutivos.

O desenvolvimento ser√° realizado utilizando o Google Colab e demais ferramentas auxiliares conforme a necessidade, fornecendo ao usu√°rio final uma interface em que ele poder√° inserir os dados do novo ve√≠culo e obter sua classifica√ß√£o estimada (quantas estrelas ele ter√°).

</br>

# üîßDocumenta√ß√£o do Projeto
O Projeto √© composto de 3 arquivos principais:
- scrappingNew.py
- CD.ipynb
- StreamlitCD_RF_Keras.ipynb

Os arquivos `.ipynb` s√£o os notebooks que podem ser executados diretamente no Google Colab. J√° o arquivo `.py` √© um script que deve ser executado diretamente. Para isso √© necess√°rio instalar as bibliotecas:
```bash
pip install -r ./requirements.txt
```  

## üìå scrappingNew.py
### Descri√ß√£o
Este script realiza web scraping no site do EURONCAP para coletar dados estruturados de elementos HTML espec√≠ficos. Ele usa a biblioteca `requests` para fazer requisi√ß√µes HTTP e o `BeautifulSoup` para processar o HTML e extrair informa√ß√µes relevantes. Os dados coletados s√£o organizados em um DataFrame do `pandas`, facilitando a an√°lise e exporta√ß√£o para outros formatos de dados. Al√©m disso, o script utiliza o `ThreadPoolExecutor` para realizar m√∫ltiplas requisi√ß√µes de forma paralela, melhorando a efici√™ncia e diminuindo o tempo de execu√ß√£o ao processar grandes conjuntos de URLs.

### Funcionalidades
- **Requisi√ß√£o de P√°ginas da Web:** Para cada URL fornecida, o script envia uma requisi√ß√£o HTTP, obtendo o conte√∫do da p√°gina HTML.
- **Parsing do HTML:** Com o `BeautifulSoup`, o script identifica elementos HTML de interesse (por exemplo, `div` com uma classe espec√≠fica) e extrai o conte√∫do desses elementos.
- **Armazenamento de Dados:** Os dados coletados s√£o organizados em um DataFrame do `pandas` para facilitar an√°lises e manipula√ß√µes adicionais.
- **Processamento Paralelo:** Atrav√©s do `ThreadPoolExecutor`, o script executa m√∫ltiplas requisi√ß√µes simultaneamente, o que √© particularmente √∫til quando se lida com um grande n√∫mero de URLs.

### Depend√™ncias
O script requer as seguintes bibliotecas Python:
- `requests` para realizar requisi√ß√µes HTTP.
- `beautifulsoup4` para parsing e extra√ß√£o de dados do HTML.
- `pandas` para organiza√ß√£o e manipula√ß√£o dos dados extra√≠dos.
- 
### Estrutura do Script

O script importa as bibliotecas necess√°rias e define uma fun√ß√£o chamada `criar_df`, que √© respons√°vel pela maior parte do processo de scraping e extra√ß√£o de dados. Al√©m disso, ele define o cabe√ßalho `User-Agent` para simular um navegador e minimizar bloqueios.

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
from concurrent.futures import ThreadPoolExecutor
```

#### Fun√ß√£o `criar_df(url)`

Esta fun√ß√£o √© o n√∫cleo do script. Ela executa os seguintes passos:

1. **Log de In√≠cio:** Exibe uma mensagem de log indicando o in√≠cio do processamento da URL, √∫til para monitoramento e debug.

2. **Configura√ß√£o do User-Agent:** Define um `User-Agent` para a requisi√ß√£o, simulando um navegador real e evitando bloqueios comuns durante o scraping.

    ```python
    headers = {'User-Agent': "Mozilla/5.0 ... Safari/537.36"}
    ```

3. **Requisi√ß√£o HTTP:** Envia uma requisi√ß√£o GET para a URL usando a biblioteca `requests`, passando o cabe√ßalho com o `User-Agent`.

    ```python
    request = requests.get(url, headers=headers)
    ```

4. **Parsing do HTML:** Ap√≥s receber a resposta, o conte√∫do HTML da p√°gina √© analisado pelo `BeautifulSoup`. A fun√ß√£o `soup.find_all` √© usada para localizar elementos espec√≠ficos de interesse (por exemplo, `div` com classes espec√≠ficas).

    ```python
    soup = BeautifulSoup(request.content, 'html.parser')
    atributos1 = soup.find_all('div', class_='...')
    ```

5. **Extra√ß√£o de Dados:** Utiliza seletores de HTML espec√≠ficos para coletar os dados desejados de cada elemento HTML identificado.

6. **Estrutura√ß√£o dos Dados:** Organiza os dados coletados em uma estrutura apropriada e os insere em um DataFrame do `pandas` para facilitar a manipula√ß√£o e an√°lise.

#### Processamento Paralelo com `ThreadPoolExecutor`

Para processar v√°rias URLs simultaneamente, o script usa `ThreadPoolExecutor`, que executa m√∫ltiplas inst√¢ncias da fun√ß√£o `criar_df` em paralelo. Isso permite que o script fa√ßa scraping em v√°rias p√°ginas da web ao mesmo tempo, economizando tempo em compara√ß√£o com uma execu√ß√£o sequencial.

```python
with ThreadPoolExecutor(max_workers=5) as executor:
    executor.map(criar_df, urls)
```

Neste exemplo, `max_workers=5` limita o n√∫mero de threads a 5, mas este valor pode ser ajustado com base na quantidade de URLs e na capacidade do sistema.


### Sa√≠da e Armazenamento dos Dados: 
O DataFrame resultante √© r exportado para um arquivo CSV, permitindo f√°cil integra√ß√£o com outras ferramentas e softwares de an√°lise de dados.

</br>

## üìå CD.ipynb
### Descri√ß√£o
Este notebook realiza uma an√°lise de dados sobre testes de colis√£o de ve√≠culos, com o objetivo de explorar e prever caracter√≠sticas de seguran√ßa dos ve√≠culos com base em v√°rias m√©tricas de seguran√ßa. O fluxo de trabalho inclui carregamento, limpeza, transforma√ß√£o de dados e treinamento e avalia√ß√£o de um modelo de aprendizado de m√°quina.

#### Funcionalidades
- **Carregamento e Limpeza de Dados:** Carrega o conjunto de dados de crash test e realiza transforma√ß√µes nas colunas, incluindo remo√ß√£o de unidades e convers√£o de tipos.
- **Tratamento de Dados Faltantes:** Identifica colunas num√©ricas com valores ausentes e os preenche usando valores aleat√≥rios dentro da faixa de m√©dia ¬± desvio padr√£o.
- **Treinamento e Avalia√ß√£o de Modelo:** Treina um classificador Random Forest para prever resultados de seguran√ßa com base em caracter√≠sticas dos ve√≠culos e gera m√©tricas de avalia√ß√£o do modelo.

### Estrutura do Notebook

1. **Importa√ß√£o das Bibliotecas**: Inclui `pandas`, `numpy` e m√≥dulos do `scikit-learn` para manipula√ß√£o de dados e aprendizado de m√°quina.
2. **Carregamento dos Dados**: L√™ os dados de um arquivo CSV e exibe as primeiras linhas para visualiza√ß√£o inicial.
3. **Limpeza e Transforma√ß√£o**:
   - Remove unidades de medidas em colunas espec√≠ficas (por exemplo, remove "kg" da coluna de peso).
   - Converte as colunas para tipos apropriados para facilitar a an√°lise.
4. **Tratamento de Valores Ausentes**:
   - Identifica colunas num√©ricas e preenche valores ausentes com n√∫meros aleat√≥rios, gerados dentro da faixa da m√©dia ¬± desvio padr√£o da coluna correspondente.
5. **Treinamento e Avalia√ß√£o de Modelo**:
   - Divide o conjunto de dados em treinamento e teste.
   - Treina um classificador `RandomForestClassifier` para prever resultados de seguran√ßa dos ve√≠culos.
   - Avalia o modelo usando m√©tricas como acur√°cia e relat√≥rio de classifica√ß√£o.

### Escolha do Modelo
O modelo escolhido, `RandomForestClassifier`, √© um m√©todo de aprendizado baseado em √°rvores de decis√£o e apresenta vantagens espec√≠ficas para an√°lise de dados de seguran√ßa de ve√≠culos:

1. **Robustez e Capacidade de Generaliza√ß√£o**: 
   O Random Forest √© composto de m√∫ltiplas √°rvores de decis√£o, o que permite uma maior capacidade de generaliza√ß√£o, reduzindo o risco de overfitting. Isso √© especialmente √∫til para dados complexos e com poss√≠veis intera√ß√µes entre vari√°veis, como os encontrados em dados de crash test.

2. **Interpreta√ß√£o e Import√¢ncia das Vari√°veis**:
   Uma caracter√≠stica essencial do Random Forest √© a capacidade de avaliar a import√¢ncia das vari√°veis, possibilitando a identifica√ß√£o das m√©tricas de seguran√ßa que mais influenciam a classifica√ß√£o de seguran√ßa dos ve√≠culos.

3. **Manejo de Dados Desbalanceados e Valores Faltantes**:
   A constru√ß√£o aleat√≥ria de amostras para cada √°rvore permite que o modelo lide bem com dados desbalanceados. Al√©m disso, o Random Forest √© relativamente robusto a valores faltantes, o que o torna uma boa escolha dado que o conjunto de dados original apresenta valores ausentes.

Essas qualidades tornam o `RandomForestClassifier` adequado para o tipo de an√°lise que estamos realizando, focada em prever com precis√£o a seguran√ßa dos ve√≠culos com base em uma s√©rie de vari√°veis complexas e possivelmente interdependentes.

### Fonte dos Dados
O notebook carrega os dados de um link para um arquivo CSV com os dados j√° coletados do web scrapping; 

</br>

## üìå StreamlitCD_RF_Keras.ipynb
### Descri√ß√£o
Esse arquivo possui o mesmo c√≥digo que `CD.ipynb` tratando-se da an√°lise dos dados e treinamento do modelo. Por√©m foram feitas as modifica√ß√µes necess√°rias para criar um frontend Streamlit

### Funcionalidades
- **Carregamento e Limpeza de Dados**: O aplicativo carrega um conjunto de dados de crash test e realiza transforma√ß√µes em colunas espec√≠ficas, incluindo a remo√ß√£o de unidades e convers√£o de valores para tipos apropriados.
- **Treinamento de Modelo Keras**: Treina um modelo de rede neural utilizando o Keras para prever a classifica√ß√£o de seguran√ßa do ve√≠culo com base em vari√°veis selecionadas, em paralelo treina um modelo do Random Forest para efeito comparativo.
- **Interface Interativa**: Permite ao usu√°rio inserir caracter√≠sticas de ve√≠culos e ver a previs√£o do modelo em tempo real.
- **Visualiza√ß√£o de Desempenho do Modelo**: Mostra a acur√°cia dos modelos e gr√°ficos de hist√≥rico de treinamento do Keras a resultados do Random Forest.

### Estrutura 
1. **Configura√ß√£o do Ambiente**:
   - Instala o Streamlit, Keras, e usa o Localtunnel para expor a aplica√ß√£o local para a internet.
  
2. **Interface do Streamlit**:
   - Define a interface interativa com Streamlit, que inclui entradas para vari√°veis como peso do ve√≠culo, nota de seguran√ßa, facilidade de instala√ß√£o de cadeirinha infantil, entre outras.
   
3. **Fun√ß√µes Principais**:
   - `load_data()`: Carrega os dados de um link CSV e faz a limpeza necess√°ria.
   - `preprocess_data(data)`: Preenche valores ausentes nas colunas num√©ricas com valores gerados aleatoriamente dentro da faixa de m√©dia ¬± desvio padr√£o.
   - `train_keras_model(data)`: Treina um modelo de rede neural com Keras, utilizando vari√°veis categ√≥ricas e num√©ricas ap√≥s pr√©-processamento, e avalia o modelo com m√©tricas de acur√°cia.
   - `user_input_features()`: Interface para coletar dados do usu√°rio sobre o ve√≠culo para previs√£o.

4. **Treinamento e Avalia√ß√£o**:
   - Exibe a acur√°cia do modelo e o gr√°fico de hist√≥rico de treinamento usando Streamlit.
   - Gera a previs√£o para as entradas fornecidas pelo usu√°rio.


</br>

# üìä Conclus√µes
## üöÄ Funcionamento e Como Utilizar
Para utilizar o script com frontend Streamlit `StreamlitCD_RF_Keras.ipynb` executa-se todas as c√©lulas. Abaixo da c√©lula `!wget -q -O - ipv4.icanhazip.com` ir√° aparecer o IP p√∫blico:
![alt text](images/image-2.png)

J√° abaixo da c√©lula `!npx localtunnel --port 8501` aparecer√° um link:
![alt text](images/image-3.png)

Ao acess√°-lo voc√™ ser√° apresentado √† interface do projeto

> Caso seja apresentado a uma p√°gina pedindo uma senha, ela ser√° o IP p√∫blico descrito acima


![alt text](images/image.png)
![alt text](images/image-1.png)
![alt text](images/image-4.png)

## üìΩÔ∏è Video

[Apresenta√ß√£o](https://youtu.be/lfoUzBCEqUY)

## üìù Artigo

[Artigo-Projeto](Documents/Artigo%20Projeto%20Semestral%20-%20Ci√™ncia%20de%20Dados.pdf)

## üõú Links do Colab

[Colab](https://colab.research.google.com/drive/1HpAjbqDpC8de666-JAOfuM6hRL4bSq3z?usp=sharing)


[Colab + Streamlit](https://colab.research.google.com/drive/1gC_xmsHEnGcACMlSZxwAi528CsnVk-4x?usp=sharing)

[Como usar Streamlit no Colab](https://medium.com/@yash.kavaiya3/running-streamlit-code-in-google-colab-involves-a-few-steps-c43ea0e8c0d9)